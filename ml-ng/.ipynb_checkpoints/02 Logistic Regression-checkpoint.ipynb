{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "\n",
    "We could use regression with a threshold but outliers will cause the model to yield a less-accurate prediction.\n",
    "\n",
    "## Binary Classification Problems\n",
    "\n",
    "We want the output of our hypothesis to always be between 0 and 1 -- which are the two classes in the binary classification problem. In order to satisfy this constraint, we will put our hypothesis from linear regression ($\\theta^Tx$) through the sigmoid (or logistic) function:\n",
    "\n",
    "\\begin{equation}\n",
    "  h_{\\theta}(x) = g(\\theta^Tx)\n",
    "\\end{equation}\n",
    "\n",
    "where $g$ is the **sigmoid** or logistic function\n",
    "\n",
    "\\begin{equation}\n",
    "  g(z) = \\frac{1}{1 + e^{-z}};\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEACAYAAABF+UbAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGyZJREFUeJzt3Xl01OXZ//H3hYpVQUXcHgFFxRVbLSJiqTguCIriXkE9\n1VL1pwgigo+2ahv6s7ZqXWpRqRsWN9RWcEGrFRsqssoqO1JEFiFCWSwuLLmeP+5gYpwkEzKZe5bP\n65zvSSb5Olyd4ufcXvfyNXdHRERyU4PYBYiIyLZTiIuI5DCFuIhIDlOIi4jkMIW4iEgOU4iLiOSw\nGkPczJ4ws5VmNqOaex40swVmNs3MjklviSIiUpVURuJDgM5V/dLMzgAOdvdDgP8HDE5TbSIiUoMa\nQ9zdxwBrqrnlHGBo2b0TgN3MbJ/0lCciItVJR0+8GbCkwutlZT8TEZF6lo4QtyQ/015+EZEM2D4N\n77EUaFHhdXNgebIbzUzhLiKyDdw92YA55RA3ko+4AV4FrgNeMLP2wFp3X1lNISn+kfWnqKiIoqKi\n2GVkBX0WgT6HcvosytXqs9i8GVavhpKS8HXVqvB19WpYswb+859wrV1bfq1bB+vXQ8OGsOuu0Lhx\n+NqoUfi+USPYZRfsySer/GNrDHEzew5IAE3N7BPg10BDwN39UXd/w8zONLOPgA3Az1L7XywikgM2\nboRly2DJEli+vPz69FNYsSJcK1eGUN5jD9hrL9hzz3A1bRp+tvfecPjhsPvu0KRJ+LrbbuHrrrvC\nDjtUX0NdQtzdL0nhnt413SMikpU2b4ZPPoGFC8P18cfhWrQIZs2CO++EffeFFi2gWTPYb79wHXNM\n+Pm++4aQbtoUttsu4+WnoyeecxKJROwSsoY+i0CfQ7m8/SzWrYM5c0Iwz50L8+aFa/Fi2GcfOPhg\nOOigcHXrBi1bkli2DM4/P0o4p8oy2aM2M8+GnriI5DH3MKKeMgWmTy+/1qyBI46AI48MXw87LFwH\nHww77hi76mqZWZUTmwpxEcltK1bA+PHhmjQJJk8O/eY2beDoo8uvli2hQW4eF6UQF5H84A4ffQT/\n+heMHg3vvRcmFNu3hxNOgHbt4Nhjw+RiHlGIi0juWrECRo2Cd94JlzucdBJ07AgnnhhWfeToCDtV\nCnERyR2lpaEl8vrrMHJk6G8nEtCpE5x2GhxyCFhV21byk0JcRLLb5s2hNfK3v8Hw4aGn3bVruDp0\nqHkddZ6rLsQLcomhiGQBdxg3Dp57Dl56KazDvuACePfdsGpEUqIQF5HM+vhjeOop+MtfYKed4NJL\nQ5gfdFDsynKSQlxE6t+mTTBiBDz6KEydCj16wMsvh12PBdbfTjeFuIjUnxUrQnD/+c9hU82118J5\n58H3vhe7sryR3+tyRCSO2bOhZ8+wO3L5cvj738Pa7h49FOBpppG4iKTP+PHw29+GnZO9e8OCBeFg\nKKk3CnERqbtx42DgwHDA1C23wIsvhklLqXcKcRHZdjNmhNCeNQtuvRWuuCI84EAyRj1xEam9xYvh\npz+F00+HM84IbZOrr1aAR6AQF5HUffEF/OpX4YTAli1h/nzo00fhHZHaKSJSM3f4619hwAD40Y/C\n+dzNm8euSlCIi0hNFi2Ca64Jz5QcOjScIChZQ+0UEUlu82a491447jg45ZRwsqACPOtoJC4i3zV3\nbpi4bNw4rP1u1Sp2RVIFjcRFpFxpKTz4YHjYQs+e4SEMCvCsppG4iATLl4fR94YNMHZsePiCZD2N\nxEUknG1y7LHhkWfvvacAzyEaiYsUsk2b4Pbb4dlnYdgwTVzmIIW4SKEqKYGLLoKdd4YpU/LuCfGF\nQu0UkUL0wQfQtm1on4wcqQDPYRqJixSaoUOhf//woIbzz49djdSRQlykUJSWhv73sGFQXAytW8eu\nSNJAIS5SCL76KhwT+8knYfOO2id5Qz1xkXy3ejWcdlo4xGrUKAV4nlGIi+SzpUvD7ssf/Qief15P\n28lDCnGRfDVvHvz4x2H7/N13QwP9656P1BMXyUeTJ8NZZ8Gdd8LPfha7GqlHCnGRfDNhApx9dlhC\neN55sauReqYQF8knY8fCuefCkCHQtWvsaiQDUmqSmVkXM5trZvPN7OYkv29hZu+a2RQzm2ZmZ6S/\nVBGp1pgxIcCHDlWAFxBz9+pvMGsAzAdOBZYDk4Du7j63wj1/Bqa4+5/N7AjgDXc/MMl7eU1/nohs\ng60j8GefhU6dYlcjaWZmuLsl+10qI/F2wAJ3X+zum4BhwDmV7ikFdi37fndg2bYWKyK1NGVK+Qhc\nAV5wUumJNwOWVHi9lBDsFQ0E3jaz64GdgdPSU56IVGvWLDjzzDCJ2aVL7GokglRCPNkQvnJPpAcw\nxN3vN7P2wDNA0oMZioqKvvk+kUiQSCRSKlREKlm4EDp3hj/8QatQ8kxxcTHFxcUp3ZtKT7w9UOTu\nXcpe3wK4u99V4Z6ZQGd3X1b2eiFwvLuvqvRe6omLpMPKldChAwwYANdcE7saqWd17YlPAlqZ2QFm\n1hDoDrxa6Z7FlLVQyiY2d6wc4CKSJp9/HlafXHaZAlxqHolDWGII/JEQ+k+4++/NbCAwyd1fLwvu\nx4BGhEnOm9x9VJL30UhcpC42bgwbeQ44IPTBLengTPJMdSPxlEI8jYUoxEW2lXs4TnbNGnj5Zdhe\ne/UKRXUhrr8FIrnijjtgzpzwQAcFuJTR3wSRXPDCC/D44+FclJ13jl2NZBG1U0Sy3fjx0K0bvPMO\n/OAHsauRCOq6OkVEYvnkk/Aw4yFDFOCSlEJcJFt98UXYTt+/vw60kiqpnSKSjdzDOnAzePppLSUs\ncFqdIpJr7rsvrEQZM0YBLtVSiItkm3fegXvu0UoUSYl64iLZZMmS0EZ5/vmwK1OkBgpxkWyxcSNc\nfDHccAOcfHLsaiRHaGJTJFv06wcffQSvvAINNL6ScprYFMl2L70EI0bA5MkKcKkVjcRFYlu4ENq3\nhzffhLZtY1cjWUg7NkWy1caN0L073H67Aly2iUbiIjENGADz54c+uNaDSxXUExfJRm++CS++CFOn\nKsBlmynERWL49FPo2TMcMdu0aexqJIepJy6SaaWl4Qk9V18NHTvGrkZynEJcJNMeegjWrYPbbotd\nieQBTWyKZNKsWZBIwNixcMghsauRHKElhiLZ4Ouv4dJL4Xe/U4BL2mgkLpIpN98M8+bB8OFajSK1\noiWGIrG9/z4MHQozZijAJa3UThGpbxs2wOWXw8MPw157xa5G8ozaKSL1rU8fWLs2PGZNZBuonSIS\ny6hR4XTCGTNiVyJ5Su0Ukfqyfj38/Ofw2GPQpEnsaiRPqZ0iUl+uvTacUvjEE7ErkRyndopIpv3z\nn/DaazBzZuxKJM+pnSKSbhs2wJVXwuDBsPvusauRPKd2iki63XgjlJTAM8/ErkTyhNopIpkybhw8\n/7zaKJIxaqeIpMvXX4fVKH/8o84Il4xRiIuky+9/Hw62uuii2JVIAVFPXCQdZs8OD3iYNg2aN49d\njeSZOh9Fa2ZdzGyumc03s5uruOcnZjbLzD40M83oSOEoLYWrroLf/EYBLhlX48SmmTUABgGnAsuB\nSWb2irvPrXBPK+Bm4AR3X29me9ZXwSJZZ/Dg8PWaa+LWIQUpldUp7YAF7r4YwMyGAecAcyvccxXw\nkLuvB3D3VekuVCQrLVsGv/oV/Otf0EBTTJJ5qfytawYsqfB6adnPKjoUOMzMxpjZWDPrnK4CRbJa\n375he/2RR8auRApUKiPxZM30yrOT2wOtgI7A/sB7ZtZ668hcJC+99hpMn64jZiWqVEJ8KSGYt2pO\n6I1Xvmecu5cCH5vZPOAQYHLlNysqKvrm+0QiQSKRqF3FItlgwwbo3TscbrXTTrGrkTxTXFxMcXFx\nSvfWuMTQzLYD5hEmNj8FJgI93H1OhXs6l/3sirJJzcnAMe6+ptJ7aYmh5IebboIVKzQKl4yo07Z7\nd99iZr2Btwk99CfcfY6ZDQQmufvr7v6WmZ1uZrOAzcCAygEukjemT4e//EVb6yUraLOPSG2UlkKH\nDmF7/ZVXxq5GCkSdN/uISJnHHw9Pq+/ZM3YlIoBG4iKpKymBo46Cd96BH/wgdjVSQKobiSvERVJ1\nxRXhdMJ7741diRQYnScuUlejR4cn18+eHbsSkW9RT1ykJps2Qa9e8MAD0Lhx7GpEvkUhLlKTBx6A\n/feH88+PXYnId6gnLlKdpUvhmGNg/Hho1Sp2NVKgtMRQZFv16wfXXacAl6yliU2Rqrz9NkyZAkOH\nxq5EpEoaiYsk8/XX4YCrBx/UAVeS1RTiIsn84Q/hjPCuXWNXIlItTWyKVPbxx9C2LXzwAbRsGbsa\nEU1sitRKv37hUoBLDtDEpkhFb7wBs2bBsGGxKxFJiUJcZKuvvoLrr4dBg2DHHWNXI5IStVNEtrrn\nnnA6YZcusSsRSZkmNkUAFi2C444L68L337/m+0UySBObIjW54Qa48UYFuOQc9cRFRo6EOXPgxRdj\nVyJSawpxKWxffQV9+8JDD2kyU3KS2ilS2O65B44+Gjp3jl2JyDbRxKYULk1mSo7QxKZIMn37Qv/+\nCnDJaeqJS2F67TWYNw9eeil2JSJ1ohCXwvPll2EUPniwJjMl56mdIoXnrrugTRs4/fTYlYjUmSY2\npbAsXAjt2sHUqeqFS87QxKYIgDv06QP/+78KcMkb6olL4RgxIjzwYcSI2JWIpI3aKVIYNmwIj1t7\n6ik4+eTY1YjUitopInfcAR06KMAl72gkLvlv7lw48USYMQP+539iVyNSaxqJS+Fyh1694LbbFOCS\nlxTikt+eew7WrIHrrotdiUi9UDtF8teaNWEyc8QIOP742NWIbLPq2ikKcclfvXqFdsojj8SuRKRO\n6twTN7MuZjbXzOab2c3V3HehmZWaWZttLVYkLSZOhOHD4c47Y1ciUq9qDHEzawAMAjoDrYEeZnZ4\nkvsaAX2A8ekuUqRWNm+Ga6+Fu++GJk1iVyNSr1IZibcDFrj7YnffBAwDzkly3/8H7gK+TmN9IrU3\naBDsthtcdlnsSkTqXSoh3gxYUuH10rKffcPMjgGau/sbaaxNpPaWLAkbewYPBkvaQhTJK6mcnZLs\n34RvZifNzID7gctr+GdE6l+fPnD99XDoobErEcmIVEJ8KVDxyLfmwPIKrxsTeuXFZYG+L/CKmXVz\n9ymV36yoqOib7xOJBIlEovZViyQzYkTYnfnCC7ErEamT4uJiiouLU7q3xiWGZrYdMA84FfgUmAj0\ncPc5Vdz/T+BGd5+a5HdaYij14/PPw5rwZ56Bk06KXY1IWtVpiaG7bwF6A28Ds4Bh7j7HzAaa2VnJ\n/hHUTpFMu/VW6NRJAS4FR5t9JPeNGwcXXAAzZ8Iee8SuRiTtdACW5K+NG+Gqq+CBBxTgUpAU4pLb\n7roLDjwQLroodiUiUaidIrlr6znhU6ZAixaxqxGpN2qnSP4pLYUrr4Rf/1oBLgVNIS65adCgsCOz\nV6/YlYhEpXaK5J5//xvatYOxY7UzUwqC2imSP9xDG+WWWxTgIijEJdc8+ihs2AD9+sWuRCQrqJ0i\nuWPxYmjbFoqLoXXr2NWIZIzaKZL7SkuhZ0/o318BLlKBQlxywyOPwBdfwIABsSsRySpqp0j2W7AA\nTjgB3n8fDjssdjUiGad2iuSuLVvgiivg9tsV4CJJKMQlu917L+ywQ3hij4h8h9opkr2mToXOnWHS\nJDjggNjViESjdorkni+/hEsvhfvvV4CLVEMjcclOfftCSQk895yeWi8Fr7qReCoPShbJrLfeguHD\nYfp0BbhIDRTikl1KSsKmnqefhiZNYlcjkvXUTpHsUVoKXbtCmzbw29/GrkYka2hiU3LDfffB+vVQ\nVBS7EpGcoZG4ZIeJE+Hss8NXrUYR+RaNxCW7rVsHPXqE81EU4CK1opG4xOUOF1wA++0XHrkmIt+h\nJYaSve6/H5Yuheefj12JSE7SSFziGTMGLrwQJkxQG0WkGuqJS/YpKQl98CFDFOAidaAQl8zbtAku\nvjgcMXvGGbGrEclpaqdI5vXtCx99BK++CtttF7sakayniU3JHk89BW++GdaDK8BF6kwjccmciRPh\nrLNg9Gg44ojY1YjkDE1sSnzLloX14I89pgAXSSOFuNS/DRvClvpeveCcc2JXI5JX1E6R+rVlSxiB\nN2kCTz6p88FFtoEmNiWeW26BtWvhxRcV4CL1QCEu9WfwYBgxAsaPh4YNY1cjkpdS6ombWRczm2tm\n883s5iS/72dms8xsmpn9w8xapL9UySnDh8NvfgN//zs0bRq7GpG8VWNP3MwaAPOBU4HlwCSgu7vP\nrXDPScAEd//KzK4BEu7ePcl7qSdeCMaMgfPOCwF+7LGxqxHJeXVdYtgOWODui919EzAM+NYSA3cf\n7e5flb0cDzSrS8GSw2bNChOZzz6rABfJgFRCvBmwpMLrpVQf0j8H3qxLUZKj/v1v6NIF7r0XTj89\ndjUiBSGVic1kQ/ikPREzuww4FjipqjcrqvD8xEQiQSKRSKEEyXpLl8Jpp8EvfwmXXRa7GpGcVlxc\nTHFxcUr3ptITbw8UuXuXste3AO7ud1W67zTgj0BHd19dxXupJ56PSkqgY0e48koYMCB2NSJ5p649\n8UlAKzM7wMwaAt2BVyv9AT8EBgPdqgpwyVOrV4fWycUXK8BFIqgxxN19C9AbeBuYBQxz9zlmNtDM\nziq77W5gF+AlM5tqZiPqrWLJHp99BqecAp07Q4U2mYhkjrbdy7ZZuRJOPTWchXLHHdqNKVKPdIqh\npNeKFXDyyWEpoQJcJCqFuNTOokXw4x/DJZfAwIEKcJHIFOKSug8/hBNPhH794LbbYlcjIugALEnV\nuHFw7rnwwAPhKfUikhU0EpeavfwydOsWno+pABfJKhqJS9Xc4b774P77dZiVSJZSiEtymzdDnz7w\n/vuhldJCpwuLZCOFuHzXZ5/BT34C3/teOFZ2111jVyQiVVBPXL5tyhQ47jho3x5ef10BLpLlNBKX\nckOHQv/+8PDDcNFFsasRkRQoxAU2bIDevUPv+9134fvfj12RiKRI7ZRCN2sWtGsHW7bABx8owEVy\njEK8UJWWwp/+BIkE3HRTaKU0ahS7KhGpJbVTCtGSJdCzJ3z+OYwdC4ccErsiEdlGGokXEncYMiRs\n2kkkwvJBBbhITtNIvFB89BFccw2sWQNvvQU//GHsikQkDTQSz3dffw2/+11Y933GGTBhggJcJI9o\nJJ7P3ngDbrgBDjsMJk6Egw6KXZGIpJlCPB/NmRNWnMyfH46OPfPM2BWJSD1ROyWfLF8OV18NHTuG\nicsPP1SAi+Q5hXg+WLUKfvGLsFFn993DCHzAANhxx9iViUg9U4jnsq3hfdhhsHYtTJ0Kd98NTZrE\nrkxEMkQhnosWLYLrr4dDDy0P70cegf33j12ZiGSYQjxXuMP48dC9ezgqduedYeZMhbdIgdPqlGz3\n5ZcwbBgMGgTr1sF118Fjj0HjxrErE5EsYO6euT/MzDP55+Us9/BwhiefDAHevn04KrZzZ2ig/3gS\nKTRmhrtbst9pJJ5NliyBF16AZ54Jo+6ePUO/W+0SEamCRuKxLV8OI0aE8J45E84/Hy65BE46SaNu\nEQGqH4krxDPNHebOhZEjYfjwsLuya1e44IJwtonWdotIJQrx2D7/HEaPDqcHjhwJmzaF4D73XDjl\nFGjYMHaFIpLF1BPPtC++CM+rHD06PLNy2jQ4/njo1AleeQWOOgos6f8fIiK1opF4OixdGtZwjxsX\nrhkz4Oijy88wOfHEsK5bRGQbqJ2SLu4hsKdPh8mTw/XBB7BxI5xwQvl1/PEKbRFJG4V4bbmHc0nm\nzIHZs8MT4WfODOG9ww5hlN2mDbRtG64DDlB7RETqjUI8GXf47LNwDsnCheHxZQsXhhMA580Lvz/8\ncGjduvw6+mjYZ5/YlYtIgalziJtZF+ABwlkrT7j7XZV+3xAYChwLrAIudvdPkrxPZkLcPRwMtWxZ\nWIe9fHnYSLP1Wrw4XDvtBC1bwsEHQ6tW4euhh4ZTAffcU6NrEckKdQpxM2sAzAdOBZYDk4Du7j63\nwj3XAt93915mdjFwnrt3T/Je2xbipaVhB+N//hOuVavKr88+g5KS8HXFinCtXBnWWzdrBvvtF64W\nLb65iktKSFx4oc4fAYqLi0kkErHLiE6fQzl9FuWy5bOo6xLDdsACd19c9mbDgHOAuRXuOQf4ddn3\nfwUGVfluo0bBf/8b1k5XvNavD0Fd8Vq7Njydff162GUX2GOPcO21FzRtGkbLe+8N7dqFn+27b7j2\n2afaicXioiISCnAge/6SxqbPoZw+i3K58FmkEuLNgCUVXi8lBHvSe9x9i5mtNbM93P0/33m3O++E\nRo1CKO+6axgNN24MBx4Iu+0WfrbbbuEJNRWv7bWkXUSkslSSMdkQvnJPpPI9luSeYNSoFP5IERFJ\nRSo98fZAkbt3KXt9C+AVJzfN7M2yeyaY2XbAp+6+d5L3ypKlKSIiuaUuPfFJQCszOwD4FOgO9Kh0\nz2vA5cAE4CLg3doUISIi26bGEC/rcfcG3qZ8ieEcMxsITHL314EngKfNbAGwmhD0IiJSzzK62UdE\nRNKroJ86YGYDzKzUzPaIXUssZna3mc0xs2lm9jcz2zV2TZlmZl3MbK6ZzTezm2PXE4uZNTezd81s\ntpl9aGbXx64pNjNrYGZTzOzV2LVUpWBD3MyaA6cBi2PXEtnbQGt3PwZYAPwicj0ZVbaZbRDQGWgN\n9DCzw+NWFc1m4EZ3PxI4AbiugD+LrfoCs2MXUZ2CDXHgfuCm2EXE5u7vuHtp2cvxQPOY9UTwzWY2\nd98EbN3MVnDcfYW7Tyv7/r/AHMIekIJUNtA7E3g8di3VKcgQN7OzgSXu/mHsWrJMT+DN2EVkWLLN\nbAUbXFuZWUvgGMKKs0K1daCX1ROHebsN0sz+AVQ8cnDrBqTbgF8CnSr9Lm9V81nc6u6vld1zK7DJ\n3Z+LUGJMqWxmKyhm1ohwfEbfshF5wTGzrsBKd59mZgmyOCPyNsTdvVOyn5vZUUBLYLqZGaF9MNnM\n2rl7SQZLzJiqPoutzOxywn82npKZirLKUmD/Cq+bEw56K0hmtj0hwJ9291di1xNRB6CbmZ0J7AQ0\nNrOh7v7TyHV9R8EvMTSzRUAbd18Tu5YYyo4Zvhfo6O6rY9eTaWU7jOcRTun8FJgI9HD3OVELi8TM\nhgKr3P3G2LVkCzM7Cejv7t1i15JMQfbEK3Gy+D+VMuBPQCPgH2VLqR6OXVAmufsWYOtmtlnAsAIO\n8A7ApcApZja17O9Dl9h1SfUKfiQuIpLLNBIXEclhCnERkRymEBcRyWEKcRGRHKYQFxHJYQpxEZEc\nphAXEclhCnERkRz2fzrA8NXB/AsKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ddafde7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "x=np.linspace(-5, 5, 100)\n",
    "y=list(map(sigmoid, x))\n",
    "plt.plot(x,y)\n",
    "_ = plt.xlim([-5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the logistic regression model of\n",
    "\n",
    "\\begin{equation}\n",
    "  h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}.\n",
    "\\end{equation}\n",
    "\n",
    "## Interpreting the Hypothesis\n",
    "The output of the hypothesis is interpreted as the probability that y = 1, given x, parameterized by theta:\n",
    "\n",
    "\\begin{equation}\n",
    "  h_{\\theta}(x) = P(y = 1 \\mid x;\\theta).\n",
    "\\end{equation}\n",
    "\n",
    "It is a property of the logistic function that $g(z) \\geq 0.5$ when $z \\geq 0$. If we assume the we'll predict $y = 1$ for $h_{\\theta}(x) \\geq 0.5$ and $y = 0$ otherwise, we will predict $y = 1$ for $\\theta^Tx \\geq 0$, and $y = 0$ for $\\theta^Tx < 0$.\n",
    "\n",
    "## Decision Boundary\n",
    "\n",
    "For an input in two dimensions, the **decision boundary** is that line that separates the region of the $x_1x_2$ plane where we predict $y = 0$ from the region where we predict $y = 1$. For values of $x_1$ and $x_2$ that fall on this line, $h_{\\theta}(x) = 0.5$.\n",
    "\n",
    "The decision boundary is a property of the hypothesis and the parameters $\\theta$, and *not* of the dataset. It is an equation that we can arrive at once we've set all of our parameters for our model. The training set is used to fit $\\theta$.\n",
    "\n",
    "We are not limited to linear hypotheses: higher-order polynomial hypotheses will yield non-linear decision boundaries, depending on the particular polynomial/model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "We can't use the same cost function for logistic regression as we used for linear regression because that function will cause the output to be non-convex, i.e. have more than one optimum. Instead we can use\n",
    "\n",
    "\\begin{equation}\n",
    "    J(\\theta) = \\frac{1}{m} \\sum_{i = 1}^{m} Cost(h_{\\theta}(x^{(i)}),y^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "  Cost(h_{\\theta}(x),y) =\n",
    "    \\begin{cases}\n",
    "      -\\log(h_{\\theta}(x)) & \\text{if y = 1}\\\\\n",
    "      -\\log(1 - h_{\\theta}(x)) & \\text{if y = 0}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "This function is guaranteed to be convex. The more wrong a hypothesis is, the higher the cost; a prediction of $h_{\\theta}(x) = 0.5$ has a non-zero cost in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEXCAYAAACtTzM+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwczMMg4EMzMDHAhmZpZxIJiZGQD/HxgMltYcNx8GAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ddb113f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0.00001, 1, 100, endpoint=False)\n",
    "y_1 = -np.log(x)\n",
    "y_0 = -np.log(1 - x)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "fig.text(0.5, 0.04, 'Hypothesis Output', ha='center')\n",
    "ax1.plot(x, y_1)\n",
    "ax1.set_title('y = 1')\n",
    "ax1.set_ylabel('Cost')\n",
    "ax2.plot(x, y_0)\n",
    "_ = ax2.set_title('y = 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a cool way to combine the if'd cost function above into one\n",
    "\n",
    "\\begin{equation}\n",
    "    Cost(h_{\\theta}(x),y) =\n",
    "      -y\\log(h_{\\theta}(x))\n",
    "      - \n",
    "      (1 -y)\\log(1 - h_{\\theta}(x))\n",
    "\\end{equation}\n",
    "\n",
    "such that if $y = 0$ the first term will be multiplied by 0, canceling it out, and if $y = 1$ the second term will be multiplied by 0, canceling _it_ out! Pretty sweet. For binary classification problems, we know that $y \\in {0, 1}$.\n",
    "\n",
    "This cost function can be derived by statistics using the principal of **maximum likelihood estimation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "As before (in linear regression) our goal is to fit the parameters $\\theta$ by minimizing the cost function $J(\\theta)$ so as to be able to make a prediction given a new $x$ using\n",
    "\\begin{equation}\n",
    "  h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}.\n",
    "\\end{equation}\n",
    "\n",
    "Our parameter update function for a gradient descent step will have the same form as before:\n",
    "\\begin{equation}\n",
    "    \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)\n",
    "\\end{equation}\n",
    "where we simultaneously update all $\\theta_j$.\n",
    "\n",
    "The partial derivative of the cost function is\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = \n",
    "        \\sum_{i=1}^m = (h_{\\theta}(x^{(i)}) - y^{(i)})\n",
    "            x^{(i)}_j\n",
    "\\end{equation}\n",
    "\n",
    "which is the same as it was for linear regression! But the definition of the hypothesis _has_ changed.\n",
    "\n",
    "As with linear regression\n",
    "* we can check that gradient descent is working by plotting $J(\\theta)$ and observing it decreasing for each subsequent iteration of the algorithm.\n",
    "* feature scaling can be used to speed up the algorithm.\n",
    "\n",
    "# Vectorized Implementations\n",
    "## Cost Function\n",
    "\\begin{equation}\n",
    "    h = g(X\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    J(\\theta) = \\frac{1}{m} \\cdot (\n",
    "        -y^T\\log(h) - (1 - y)^T(\\log(1 - h)\n",
    "    )\n",
    "\\end{equation}\n",
    "## Gradient Descent\n",
    "\\begin{equation}\n",
    "    \\theta := \\theta - \\alpha \\frac{1}{m}X^T(g(X\\theta) - \\hat y)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you are the administrator of a university department and\n",
    "you want to determine each applicant’s chance of admission based on their\n",
    "results on two exams. You have historical data from previous applicants\n",
    "that you can use as a training set for logistic regression. For each training\n",
    "example, you have the applicant’s scores on two exams and the admissions\n",
    "decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam1</th>\n",
       "      <th>exam2</th>\n",
       "      <th>admitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.623660</td>\n",
       "      <td>78.024693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.286711</td>\n",
       "      <td>43.894998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.847409</td>\n",
       "      <td>72.902198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.182599</td>\n",
       "      <td>86.308552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.032736</td>\n",
       "      <td>75.344376</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       exam1      exam2  admitted\n",
       "0  34.623660  78.024693         0\n",
       "1  30.286711  43.894998         0\n",
       "2  35.847409  72.902198         0\n",
       "3  60.182599  86.308552         1\n",
       "4  79.032736  75.344376         1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('machine-learning-ex2/ex2/ex2data1.txt', header=None)\n",
    "df.columns = ['exam1', 'exam2', 'admitted']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = df[['exam1', 'exam2']], df['admitted']\n",
    "m = X.shape[0] # number of examples\n",
    "X.insert(0, 'intercept', np.ones(m))\n",
    "n = X.shape[1] # number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAFgCAYAAADuCe0ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ddacc6b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.lmplot('exam1', 'exam2', data=df,\n",
    "               hue='admitted', palette=[\"red\", \"blue\"],\n",
    "               legend=False, fit_reg=False)\n",
    "g.ax.legend(labels=['Not Admitted','Admitted'], loc=0, bbox_to_anchor=(1.3,1))\n",
    "\n",
    "plt.xlabel('Exam 1 Score')\n",
    "_ = plt.ylabel('Exam 2 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(theta, X):\n",
    "    return sigmoid(X.dot(theta))\n",
    "\n",
    "def compute_cost(theta, X, y):\n",
    "    h = hypothesis(theta, X)\n",
    "    term1 = -y * np.log(h) # cancelled if y == 0\n",
    "    term2 = -(1-y) * np.log(1-h) # cancelled if y == 1\n",
    "    return sum(term1 + term2)/m\n",
    "\n",
    "def compute_gradient(theta, X, y):\n",
    "    h = hypothesis(theta, X)\n",
    "    return (X.T.dot((h - y)))/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599458"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With initial values of theta = 0 you should see that the cost is about 0.693.\n",
    "theta = np.zeros(n)\n",
    "compute_cost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Optimization Techniques\n",
    "Gradient descent isn't the only way to optimize the parameters $\\theta$; examples of other optimization algorithms are\n",
    "* [Conjugate Descent](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
    "* [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "* [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)\n",
    "\n",
    "The advantages of these other algorithms is that we don't need to pick a learning rate $\\alpha$ and they run more quickly than gradient descent; however, the algorithms are more complex.\n",
    "\n",
    "They are useful for larger machine learning problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(compute_cost(X, y, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (100,) not aligned: 3 (dim 0) != 100 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-29010cc7054b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TNC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                   options=options)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[0;32m--> 453\u001b[0;31m                              **options)\n\u001b[0m\u001b[1;32m    454\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cobyla'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cobyla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/tnc.py\u001b[0m in \u001b[0;36m_minimize_tnc\u001b[0;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, **unknown_options)\u001b[0m\n\u001b[1;32m    407\u001b[0m                                         \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxCGit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                                         \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepmx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                                         xtol, pgtol, rescale, callback)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mfunv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjacv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/optimize/tnc.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-316-f771e2749cb8>\u001b[0m in \u001b[0;36mcompute_cost\u001b[0;34m(X, y, theta)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mterm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cancelled if y == 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mterm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# cancelled if y == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-316-f771e2749cb8>\u001b[0m in \u001b[0;36mhypothesis\u001b[0;34m(X, theta)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,) and (100,) not aligned: 3 (dim 0) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as optimize\n",
    "\n",
    "# You should see that the cost is about 0.203.\n",
    "# This final θ value will then be used to plot the decision boundary on the training data.\n",
    "options= {'maxiter': 400}\n",
    "optimize.minimize(fun=compute_cost,\n",
    "                  x0=theta,\n",
    "                  args=(X, y),\n",
    "                  jac=compute_gradient,\n",
    "                  method='TNC',\n",
    "                  options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta:\n",
      " intercept    0.001000\n",
      "exam1        0.120092\n",
      "exam2        0.112628\n",
      "dtype: float64\n",
      "Cost:\n",
      " 4.960600143358933\n",
      "\n",
      "Theta:\n",
      " intercept   -0.003000\n",
      "exam1       -0.088034\n",
      "exam2       -0.105849\n",
      "dtype: float64\n",
      "Cost:\n",
      " 8.645471645494405\n",
      "\n",
      "Theta:\n",
      " intercept    0.003000\n",
      "exam1        0.360264\n",
      "exam2        0.337872\n",
      "dtype: float64\n",
      "Cost:\n",
      " nan\n",
      "\n",
      "Theta:\n",
      " intercept   -0.001000\n",
      "exam1        0.152135\n",
      "exam2        0.119390\n",
      "dtype: float64\n",
      "Cost:\n",
      " 5.774430398356618\n",
      "\n",
      "Theta:\n",
      " intercept   -0.005000\n",
      "exam1       -0.055993\n",
      "exam2       -0.099091\n",
      "dtype: float64\n",
      "Cost:\n",
      " 6.910500822001945\n",
      "\n",
      "Theta:\n",
      " intercept    0.000998\n",
      "exam1        0.392222\n",
      "exam2        0.344551\n",
      "dtype: float64\n",
      "Cost:\n",
      " nan\n",
      "\n",
      "Theta:\n",
      " intercept   -0.003002\n",
      "exam1        0.184092\n",
      "exam2        0.126070\n",
      "dtype: float64\n",
      "Cost:\n",
      " 6.584694094674066\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikhailgolubitsky/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6931471805599458"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"Runs gradient descent algorithm for given input matrix and target vectors.\n",
    "\n",
    "    Args:\n",
    "        X (matrix): The m (number of examples) x n (number of features) input matrix.\n",
    "        y (vector): The target vector.\n",
    "        alpha (str): The learning rate.\n",
    "        num_iters (str): How many times to run gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        (np.array, np.array): The parameters theta and the history of the cost function.\n",
    "\n",
    "    \"\"\"\n",
    "    J_history = np.zeros(num_iters)\n",
    "    m = X.shape[0]\n",
    "        \n",
    "    for iteration in range(num_iters):\n",
    "        theta = theta - alpha * compute_gradient(X, y, theta)\n",
    "        print(f'Theta:\\n {theta}')\n",
    "        print(f'Cost:\\n {compute_cost(X, y, theta)}\\n')\n",
    "        # Save the cost J in every iteration for visualization  \n",
    "        J_history[iteration]= compute_cost(X, y, theta)\n",
    "        \n",
    "    return (theta, J_history)\n",
    "\n",
    "alpha = 0.01\n",
    "num_iters = 7\n",
    "gradient_descent(X, y, theta, alpha, num_iters)\n",
    "compute_cost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification Problems\n",
    "Examples:\n",
    "* Email tagging: Work, Friends, Family\n",
    "* Medical diagrams: Not ill, cold, flu\n",
    "* Weather: Sunny, Cloudy, Rain, Snow\n",
    "\n",
    "Each of the classes are represented with discrete values of $y$, for $y \\in {0..n}$.\n",
    "\n",
    "## One-vs-all (One-vs-rest)\n",
    "\n",
    "We can treat a multiclass classification problem as a collection of binary classifiers that find the probability that a given example is of a particular class. To do this, we convert our class values to $y = 0$ for all classes that aren't the target class, and to $y = 1$ for the target class. We then take the maximum of each of these binary classifiers, yielding the most confident value as our final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem Of Overfitting\n",
    "Linear regression and logistic regression are both prone to **overfitting**. Overfitting describes the situation when our algorithm has arrived at parameters $\\theta$ that so precisely predict values of $y$ for the training set that they learn the noise in the training data and actually end up yielding poor results for predictions of $y$ outside the training set (i.e. the algorithm **generalizes** poorly).\n",
    "\n",
    "We are particularly prone to overfitting when we have a high number of features and a small training set. The more features we add, the better performance we are likely to get on the training set, but the more likely we are to not generalize well.\n",
    "\n",
    "## Bias-variance Tradeoff\n",
    "We can also **underfit** the data, where the algorithm doesn't \"learn enough\" about the data. This is also known as \"the algorithm has **high bias**.\" When we overfit the data, this is also known as \"the algorithm has **high variance**.\" This tradeoff between over- and underfitting is known as the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).\n",
    "\n",
    "## What To Do About Overfitting\n",
    "\n",
    "1. Reduce the number of features\n",
    "    * Manually select which features to keep\n",
    "    * Model selection algorithms\n",
    "2. Regularization\n",
    "    * Keep all the features, but reduce the magnitude of parameters $\\theta_j$\n",
    "    * Works well when we have a lot of features that each contribute to predicting $y$\n",
    "    \n",
    "## Regularization\n",
    "We want to reduce the complexity of the model to avoid overfitting. One way to do this is to decrease the value of $\\theta_j$ by \"penalizing\" large values. Removing a higher-order term will greatly simplify the model (e.g. removing $x^2$ from a quadratic equation will yield a linear model, which is simpler), and decreasing the coefficient of a higher-order term will moderately simplify the model. We don't want to penalize _too_ much because that will cause underfitting (e.g. if we remove all the higher order terms we get a linear model, which may not fit the data well at all).\n",
    "\n",
    "Goals:\n",
    "1. Fit the training data well to avoid underfitting\n",
    "2. Keep the parameters small to avoid overfitting\n",
    "\n",
    "The **regularization parameter**, lambda ($\\lambda$), controls the tradeoff between these goals. If it's set too low, it's as though the extra term in the cost function $J$ weren't there, and so we'll end up overfitting. If it's set too high, we'll penalize all the parameters too much and we'll end up underfitting by removing the effects of all the features from the hypothesis and essentially fitting a straight line of $h_{\\theta}(x) = \\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
