{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\let\\vec\\mathbf$$\n",
    "# Components of a Neural Network\n",
    "* Input Layer\n",
    "* Hidden Layer(s) - anything that isn't an input or output layer\n",
    "* Output layer\n",
    "\n",
    "## Notation\n",
    "| Symbol | Meaning   |\n",
    "|:-:|:-:|\n",
    "|   $x$  | inputs |\n",
    "|   $x_0$  | bias input |\n",
    "|   $x_n$  | input node $n$ |\n",
    "|   $\\theta$  | weights/parameters |\n",
    "|   $a^{(j)}_i$  | the \"activation\" (value computed by the neuron) of the $i^{th}$ neuron in layer $j$ |\n",
    "|   $\\Theta^{(j)}$  | matrix of weights controlling the function mapping from layer $j$ to layer $j + 1$ |\n",
    "|   $\\Theta^{(j)}_{tf}$  | value of the weight **to** the $t$ node in layer $j + 1$ **from** the $f$ node in layer $j$  |\n",
    "\n",
    "## The Matrix $\\Theta^{(j)}$ \n",
    "$\\Theta^{(j)}$ is a matrix having dimensions $s_{j + 1} \\times s_j + 1$, adding 1 to the columns for the bias input to layer $j + 1$. So for an input layer of 3 nodes (features) and a hidden layer of 4 nodes, we would have $\\Theta^{(1)}_{4 * 4}$. In other words, a matrix having dimension (number of nodes in next layer) times (number of nodes in this layer + 1 for the bias).\n",
    "\n",
    "Each row in $\\Theta$ represents the weights \n",
    "* to one node in the next layer\n",
    "* from all nodes in this layer (including the bias node)\n",
    "\n",
    "Each column in $\\Theta$ represents the weights \n",
    "* to all nodes in the next layer\n",
    "* from one node in this layer (possibly the bias node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "### Applying $\\Theta^{(j)}$ to Layer $J$\n",
    "As an example, let's take an input layer having 3 nodes that is connected to a hidden layer of 2 nodes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\theta_{\\text{11}} & \\theta_{\\text{12}} & \\theta_{\\text{13}} \\\\ \n",
    "\\theta_{\\text{21}} & \\theta_{\\text{22}} & \\theta_{\\text{23}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We also have input $\\vec{x}$\n",
    "\n",
    "$$\n",
    "\\vec{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2 \\\\ x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can calculate the activations of layer 2 for the first node\n",
    "\n",
    "$$\n",
    "a^{(2)}_1 = g(\\Theta^{(1)}_{11} \\cdot x_1 + \\Theta^{(1)}_{12} \\cdot x_2 + \\Theta^{(1)}_{13} \\cdot x_3)\n",
    "$$\n",
    "\n",
    "and for the second node\n",
    "\n",
    "$$\n",
    "a^{(2)}_2 = g(\\Theta^{(1)}_{21} \\cdot x_1 + \\Theta^{(1)}_{22} \\cdot x_2 + \\Theta^{(1)}_{23} \\cdot x_3)\n",
    "$$\n",
    "\n",
    "where $g$ is a sigmoid or other activation function.\n",
    "\n",
    "We can express the above as a dot product:\n",
    "\n",
    "$$\n",
    "    \\vec{a^{j+1}} = g(\\Theta^{(j)}\\vec{x})\n",
    "$$\n",
    "\n",
    "This dot product represents the application of each of the weights in layer $j$ to each of the inputs to layer $j$ -- yielding a vector of the size of the number of nodes in the next layer ($s_{j + 1}$).\n",
    "\n",
    "As a final step in generating $\\vec{a^{j+1}}$, we have to prepend a bias node of $a^{j+1}_0 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification Using Neural Networks\n",
    "We can use an extension of the one-vs-all method. We can build a neural network having an output layer with n nodes for each of the classes. For three possible classes we would have target vectors of\n",
    "$$\n",
    "y_1 = \\begin{bmatrix}\n",
    "1 \\\\ 0 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "\\hspace{1cm}\n",
    "y_2 = \\begin{bmatrix}\n",
    "0 \\\\ 1 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "\\hspace{1cm}\n",
    "y_3 = \\begin{bmatrix}\n",
    "0 \\\\ 0 \\\\ 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "instead of\n",
    "\n",
    "$$\n",
    " y \\in \\{0, 1, 2\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backward Propagation\n",
    "## Notation\n",
    "| Symbol | Meaning   |\n",
    "|:-:|:-:|\n",
    "|   $L$  | Total number of layers in the network |\n",
    "| $s_l$ | Number of nodes in layer $l$, **not** counting bias |\n",
    "| $K$ | Number of output units (for multiclass classification) |\n",
    "| $\\delta^{(l)}_j$ | \"Error\" of node $j$ in layer $l$ |\n",
    "| $\\Delta^{(l)}_{xy}$ | Has shape $s_{l + 1} \\times (s_l + 1)$, same as $\\Theta^{(l)}$;<p>stores the partial derivatives of all weights $y$ going forward from neuron $x$ in layer $l$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "Training Set $\\{\n",
    "    (x^{(1)},y^{(1)}), \\ldots, (x^{(m)},y^{(m)})\n",
    "\\}$\n",
    "\n",
    "Set $\\Delta^{(l)}_{pq}$ for all $l, p, q$ where $q$ is the $q^{th}$ neuron in layer $l$ and $p$ enumerates the weights leading to all the neurons in the next layer\n",
    "\n",
    "For $i = 1$ to $m$  \n",
    "* Set $a^{(1)} = x^{(i)}$\n",
    "  * i.e. set the \"activations\" for first layer using the $i^{th}$ training example\n",
    "* Perform forward prop to compute $a^{(l)}$ for $l = 2,3,\\ldots,L$\n",
    "* Using $y^{(i)}$, compute $\\delta^{(L)} = a^{(L)} - y^{(i)}$\n",
    "  * We're computing the difference between the hypothesis and the prediction for each neuron in the layer (as we'll do for other layers); we don't need to use partial derivatives for this layer.\n",
    "* Use backprop to compute $\\delta^{(L - 1)}, \\delta^{(L - 2)}, \\ldots, \\delta^{(2)}$\n",
    "  * $\\delta^{(l)} = (\\Theta^{(l)})^{T}\\delta^{(l + 1)} \\circ g'(z^{(l)})$\n",
    "    * $\\Theta^{(j)}$ has shape $s_{l + 1} \\times (s_l + 1)$ so $(\\Theta^{(l)})^{T}\\delta^{(l + 1)}$ is calculating the dot product of a matrix w/shape $(s_l + 1) \\times s_{l + 1}$ and vector w/size $s_{l + 1}$ \n",
    "    * The initial $\\delta^{(l + 1)} = \\delta^{(L)}$\n",
    "    * Note that the $\\circ$ symbol means the \"element-wise\" [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)\n",
    "    * $g'$ is the derivative of the sigmoid function: $g' = \\frac{d}{dx}\\sigma = \\sigma(x) \\cdot (1 - \\sigma(x))$\n",
    "    * There is no error term for the 1st layer\n",
    "* Update $\\Delta^{(l)}_{pq} := \\Delta^{(l)}_{pq} + a^{(l)}_q\\delta^{(l + 1)}_p$\n",
    "  * This adds the current example's portion of the overall error at the $p^{th}$ neuron in layer $l + 1$ by multiplying it with the activation (value) of the current layer's neuron (i.e. \"how much of the overall error is this neuron responsible for?\")\n",
    "  * Vectorized form: $\\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l + 1)}(a^{(l)})^T$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.ones(40).reshape(4, 10)\n",
    "delta_l_plus_1 = np.ones(4)\n",
    "theta.T.dot(delta_l_plus_1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "1. Choose architecture (connectivity pattern between neurons).\n",
    "2. Randomly initialize weights (to small values near 0)\n",
    "  \n",
    "3. Implement forward propagation to compute $h_{\\Theta}(x^{(i)})$ of $x^{(i)}$\n",
    "4. Implement the cost function $J(\\Theta)$\n",
    "5. Implement backpropagation to compute partial derivatives $\\frac{\\partial}{\\partial\\Theta^{(l)}_{jk}}$\n",
    "6. Use gradient checking to compare partial derivates computed using backprop $\\frac{\\partial}{\\partial\\Theta^{(l)}_{jk}}$ vs using numerical estimate of gradient of $J(\\Theta)$\n",
    "    * Having verified backprop code, disable gradient checking before training network\n",
    "7. Using gradient descent or advanced optimization algorithm, try to minimize $J(\\Theta)$ as function of $\\Theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
